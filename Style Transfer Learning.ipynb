{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEqsV8KMK29H"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import torch\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# define the VGG\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        # load the vgg model's features\n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "    \n",
    "    def get_content_activations(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "            Extracts the features for the content loss from the block4_conv2 of VGG19\n",
    "            Args:\n",
    "                x: torch.Tensor - input image we want to extract the features of\n",
    "            Returns:\n",
    "                features: torch.Tensor - the activation maps of the block4_conv2 layer\n",
    "        \"\"\"\n",
    "        features = self.vgg[:23](x)\n",
    "        return features\n",
    "    \n",
    "    def get_style_activations(self, x):\n",
    "        \"\"\"\n",
    "            Extracts the features for the style loss from the block1_conv1, \n",
    "                block2_conv1, block3_conv1, block4_conv1, block5_conv1 of VGG19\n",
    "            Args:\n",
    "                x: torch.Tensor - input image we want to extract the features of\n",
    "            Returns:\n",
    "                features: list - the list of activation maps of the block1_conv1, \n",
    "                    block2_conv1, block3_conv1, block4_conv1, block5_conv1 layers\n",
    "        \"\"\"\n",
    "        features = [self.vgg[:4](x)] + [self.vgg[:7](x)] + [self.vgg[:12](x)] + [self.vgg[:21](x)] + [self.vgg[:30](x)] \n",
    "        return features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.vgg(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GH6fakD0LBOj"
   },
   "outputs": [],
   "source": [
    "def gram(tensor):\n",
    "    \"\"\"\n",
    "        Constructs the Gramian matrix out of the tensor\n",
    "    \"\"\"\n",
    "    return torch.mm(tensor, tensor.t())\n",
    "\n",
    "\n",
    "def gram_loss(noise_img_gram, style_img_gram, N, M):\n",
    "    \"\"\"\n",
    "        Gramian loss: the SSE between Gramian matrices of a layer\n",
    "            arXiv:1508.06576v2 - equation (4)\n",
    "    \"\"\"\n",
    "    return torch.sum(torch.pow(noise_img_gram - style_img_gram, 2)).div((np.power(N*M*2, 2, dtype=np.float64)))\n",
    "\n",
    "\n",
    "def total_variation_loss(image):\n",
    "    \"\"\"\n",
    "        Variation loss makes the images smoother, defined over spacial dimensions\n",
    "    \"\"\"\n",
    "    loss = torch.mean(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:])) + \\\n",
    "        torch.mean(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def content_loss(noise: torch.Tensor, image: torch.Tensor):\n",
    "    \"\"\"\n",
    "        Simple SSE loss over the generated image and the content image\n",
    "            arXiv:1508.06576v2 - equation (1)\n",
    "    \"\"\"\n",
    "    return 1/2. * torch.sum(torch.pow(noise - image, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function along with model creating and image generation of style transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xevv1Tp2Ln1U"
   },
   "outputs": [],
   "source": [
    "def main(style_img_path: str,\n",
    "         content_img_path: str, \n",
    "         img_dim: int,\n",
    "         num_iter: int,\n",
    "         style_weight: int,\n",
    "         content_weight: int,\n",
    "         variation_weight: int,\n",
    "         print_every: int,\n",
    "         save_every: int):\n",
    "\n",
    "    assert style_img_path is not None\n",
    "    assert content_img_path is not None\n",
    "\n",
    "    # define the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # read the images\n",
    "    style_img = Image.open(style_img_path)\n",
    "    cont_img = Image.open(content_img_path)\n",
    "    \n",
    "    # define the transform\n",
    "    transform = transforms.Compose([transforms.Resize((img_dim, img_dim)),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    # get the tensor of the image\n",
    "    content_image = transform(cont_img).unsqueeze(0).to(device)\n",
    "    style_image = transform(style_img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # init the network\n",
    "    vgg = VGG().to(device).eval()\n",
    "    \n",
    "    # replace the MaxPool with the AvgPool layers\n",
    "    for name, child in vgg.vgg.named_children():\n",
    "        if isinstance(child, nn.MaxPool2d):\n",
    "            vgg.vgg[int(name)] = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "    # lock the gradients\n",
    "    for param in vgg.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # get the content activations of the content image and detach them from the graph\n",
    "    content_activations = vgg.get_content_activations(content_image).detach()\n",
    "    \n",
    "    # unroll the content activations\n",
    "    content_activations = content_activations.view(512, -1)\n",
    "    \n",
    "    # get the style activations of the style image\n",
    "    style_activations = vgg.get_style_activations(style_image)\n",
    "    \n",
    "    # for every layer in the style activations\n",
    "    for i in range(len(style_activations)):\n",
    "\n",
    "        # unroll the activations and detach them from the graph\n",
    "        style_activations[i] = style_activations[i].squeeze().view(style_activations[i].shape[1], -1).detach()\n",
    "\n",
    "    # calculate the gram matrices of the style image\n",
    "    style_grams = [gram(style_activations[i]) for i in range(len(style_activations))]\n",
    "    \n",
    "    # generate the Gaussian noise\n",
    "    noise = torch.randn(1, 3, img_dim, img_dim, device=device, requires_grad=True)\n",
    "    \n",
    "    # define the adam optimizer\n",
    "    # pass the feature map pixels to the optimizer as parameters\n",
    "    adam = optim.Adam(params=[noise], lr=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "    # run the iteration\n",
    "    for iteration in range(num_iter):\n",
    "\n",
    "        # zero the gradient\n",
    "        adam.zero_grad()\n",
    "\n",
    "        # get the content activations of the Gaussian noise\n",
    "        noise_content_activations = vgg.get_content_activations(noise)\n",
    "\n",
    "        # unroll the feature maps of the noise\n",
    "        noise_content_activations = noise_content_activations.view(512, -1)\n",
    "\n",
    "        # calculate the content loss\n",
    "        content_loss_ = content_loss(noise_content_activations, content_activations)\n",
    "\n",
    "        # get the style activations of the noise image\n",
    "        noise_style_activations = vgg.get_style_activations(noise)\n",
    "\n",
    "        # for every layer\n",
    "        for i in range(len(noise_style_activations)):\n",
    "\n",
    "            # unroll the the noise style activations\n",
    "            noise_style_activations[i] = noise_style_activations[i].squeeze().view(noise_style_activations[i].shape[1], -1)\n",
    "\n",
    "        # calculate the noise gram matrices\n",
    "        noise_grams = [gram(noise_style_activations[i]) for i in range(len(noise_style_activations))]\n",
    "\n",
    "        # calculate the total weighted style loss\n",
    "        style_loss = 0\n",
    "        for i in range(len(style_activations)):\n",
    "            N, M = noise_style_activations[i].shape[0], noise_style_activations[i].shape[1]\n",
    "            style_loss += (gram_loss(noise_grams[i], style_grams[i], N, M) / 5.)\n",
    "\n",
    "        # put the style loss on device\n",
    "        style_loss = style_loss.to(device)\n",
    "            \n",
    "        # calculate the total variation loss\n",
    "        variation_loss = total_variation_loss(noise).to(device)\n",
    "\n",
    "        # weight the final losses and add them together\n",
    "        total_loss = content_weight * content_loss_ + style_weight * style_loss + variation_weight * variation_loss\n",
    "\n",
    "        if iteration % print_every == 0:\n",
    "            print(\"Iteration: {}, Content Loss: {:.3f}, Style Loss: {:.3f}, Var Loss: {:.3f}\".format(iteration, \n",
    "                                                                                                     content_weight * content_loss_.item(),\n",
    "                                                                                                     style_weight * style_loss.item(), \n",
    "                                                                                                     variation_weight * variation_loss.item()))\n",
    "\n",
    "        # create the folder for the generated images\n",
    "        if not os.path.exists('./generated/'):\n",
    "            os.mkdir('./generated/')\n",
    "        \n",
    "        # generate the image\n",
    "        if iteration % save_every == 0:\n",
    "            save_image(noise.cpu().detach(), filename='./generated/iter_{}.png'.format(iteration))\n",
    "\n",
    "        # backprop\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        adam.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "XaRtt96IMkPk",
    "outputId": "7b26bd43-35c6-40bb-a2ee-5b229914e899"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-78d455ae-f59e-4b19-b8af-a2e3fd8d4bdb\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-78d455ae-f59e-4b19-b8af-a2e3fd8d4bdb\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving content_img_1.jpg to content_img_1.jpg\n",
      "Saving style_img_1.jpeg to style_img_1.jpeg\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S9uOVjojNlwI",
    "outputId": "61d71226-6fc6-4ed0-d2f9-f10b02695997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the content and style image to generate the style transfer image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "5PflNHxPPAW3",
    "outputId": "614820f7-3cd2-4fc8-9489-74e2d34b6fe7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 574673361/574673361 [00:06<00:00, 95323435.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Content Loss: 414.509, Style Loss: 3930672.407, Var Loss: 22536.318\n",
      "Iteration: 500, Content Loss: 921.538, Style Loss: 143853.063, Var Loss: 18321.807\n",
      "Iteration: 1000, Content Loss: 978.149, Style Loss: 61980.169, Var Loss: 16756.353\n",
      "Iteration: 1500, Content Loss: 965.850, Style Loss: 16721.900, Var Loss: 14522.272\n",
      "Iteration: 2000, Content Loss: 944.350, Style Loss: 8078.620, Var Loss: 11442.044\n",
      "Iteration: 2500, Content Loss: 925.219, Style Loss: 5292.014, Var Loss: 8388.789\n",
      "Iteration: 3000, Content Loss: 908.029, Style Loss: 3873.991, Var Loss: 5902.346\n",
      "Iteration: 3500, Content Loss: 887.675, Style Loss: 2974.320, Var Loss: 4252.092\n",
      "Iteration: 4000, Content Loss: 871.106, Style Loss: 2330.851, Var Loss: 3349.500\n",
      "Iteration: 4500, Content Loss: 854.210, Style Loss: 1845.992, Var Loss: 2906.943\n",
      "Iteration: 5000, Content Loss: 838.032, Style Loss: 1480.990, Var Loss: 2678.351\n",
      "Iteration: 5500, Content Loss: 819.240, Style Loss: 1202.061, Var Loss: 2542.397\n",
      "Iteration: 6000, Content Loss: 800.785, Style Loss: 983.557, Var Loss: 2447.711\n",
      "Iteration: 6500, Content Loss: 782.529, Style Loss: 811.159, Var Loss: 2373.403\n",
      "Iteration: 7000, Content Loss: 767.424, Style Loss: 676.355, Var Loss: 2310.524\n",
      "Iteration: 7500, Content Loss: 752.518, Style Loss: 574.076, Var Loss: 2257.178\n",
      "Iteration: 8000, Content Loss: 738.761, Style Loss: 499.028, Var Loss: 2213.019\n",
      "Iteration: 8500, Content Loss: 725.810, Style Loss: 446.796, Var Loss: 2176.058\n",
      "Iteration: 9000, Content Loss: 715.116, Style Loss: 408.750, Var Loss: 2145.229\n",
      "Iteration: 9500, Content Loss: 705.067, Style Loss: 384.061, Var Loss: 2120.546\n",
      "Iteration: 10000, Content Loss: 697.472, Style Loss: 365.568, Var Loss: 2100.825\n",
      "Iteration: 10500, Content Loss: 690.564, Style Loss: 354.442, Var Loss: 2086.049\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "style_img = '/content/style_img_1.jpeg'\n",
    "content_img = '/content/content_img_1.jpg'\n",
    "\n",
    "main(style_img, content_img, 512, 12000, 10e6, 10e-4, 10e3, 500, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "DWazIvW21brs",
    "outputId": "bc3e44b9-ef9f-4e50-ca9a-84afad61459b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_0.png     iter_200.png   iter_400.png   iter_600.png\n",
      "iter_1000.png  iter_3000.png  iter_5000.png  iter_7000.png\n",
      "iter_100.png   iter_300.png   iter_500.png   iter_8000.png\n",
      "iter_2000.png  iter_4000.png  iter_6000.png  iter_9000.png\n"
     ]
    }
   ],
   "source": [
    "!ls /content/generated/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9TkhXjr1nYr"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('/content/generated/iter_8000.png')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "StyleTransfer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
